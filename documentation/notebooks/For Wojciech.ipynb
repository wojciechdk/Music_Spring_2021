{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hi Wojciech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi, Wojchiech. \n",
    "Here is my code for cleaning my data - it is kind of a mess and some part is only relevant for the heart rate data I guess. \n",
    "\n",
    "I do not have your email, but please write me, if you need help: s144289@student.dtu.dk\n",
    "\n",
    "Luna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "Data cleaning by reorganizing the data by user rather than year-month. Duplicates and non-used attributes are removed in the process. \n",
    "\n",
    "* [x] Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stuff to Read Avro files\n",
    "packages = '/home/s144289/spark-avro_2.12-3.0.0.jar'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\"--jars {0} pyspark-shell \".format(packages))\n",
    "\n",
    "# temp folder to store working memory (home path of the notebook does not have a lot of memory)\n",
    "temp_folder = \"/data/work/shared/s144289/temp/\"\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession,SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, TimestampType, IntegerType, DateType, DoubleType, StructType, StructField, LongType\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Create new context\n",
    "spark = SparkSession.builder.config(\"spark.sql.files.ignoreCorruptFiles\",\"true\")\\\n",
    "                                        .config(\"spark.memory.fraction\", \"0.6\")\\\n",
    "                                        .config(\"spark.local.dir\",temp_folder)\\\n",
    "                                        .config(\"spark.driver.memory\",'5g')\\\n",
    "                                        .master(\"local[15]\").getOrCreate()\n",
    "    \n",
    "# local[5] how many cores to use (32 in total)\n",
    "# maybe increase driver memory (.config(\"spark.driver.memory\",'10g'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reorganizing and cleaning\n",
    "\n",
    "heart_rate00:\n",
    "* [x] Drop duplicates\n",
    "\n",
    "heart_rate\n",
    "* [x] Drop duplicates\n",
    "* [x] Exclude phone HR\n",
    "* [x] Exclude dates > 2019-03 and < 2015 05\n",
    "* [x] Exclude timestamps_seen < time\n",
    "* [x] Exclude timestamps_seen - time > +30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_seen: long (nullable = true)\n",
      " |-- useruuid: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- bpm: integer (nullable = true)\n",
      " |-- devices: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_original = spark.read.format(\"avro\").load('/data/src/heartrate/yearmonth=201603/')\n",
    "df_original.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 of 47 / 201903 Total time: 7271.55 s Estimated time remaining: 0.0 s    \n",
      "Done                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2142413238, 2089611309, 0, 0, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path of the data:\n",
    "heart_rate_path = '/data/src/heartrate/'\n",
    "path = '/data/work/shared/s144289/heart_rate00'\n",
    "\n",
    "Tformat = \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"\n",
    "included = [0]*5\n",
    "total_time = 0\n",
    "estimated_time = 0\n",
    "N = len(glob.glob(heart_rate_path+'*'))\n",
    "i=0\n",
    "for folder in glob.glob(heart_rate_path+'*'):\n",
    "    start_time = time.time()\n",
    "    i+=1\n",
    "    print(i,'of',N,':',folder[-6:], end = '\\r')    \n",
    "    \n",
    "    # dataframe with data (does not load until you execute something below)\n",
    "    df_original = spark.read.format(\"avro\").load(folder)\n",
    "    included[0]+=df_original.count()\n",
    "\n",
    "    # Remove duplicates\n",
    "    df = df_original.dropDuplicates()    \n",
    "    included[1]+=df.count()\n",
    "    \n",
    "#     # Split device column to multiple columns:\n",
    "#     df = df.withColumn('devices',F.explode('devices')\n",
    "#                                ).withColumn('devices_name',F.col('devices').getItem('name'))\n",
    "    \n",
    "#     # Exclude phone measures *\n",
    "#     df = df.filter(df.devices_name == F.lit('swr12')).drop('devices')\n",
    "#     included[2]+=df.count()\n",
    "    \n",
    "#     # Remove samples out of daterange:\n",
    "#     df = df.filter( F.to_date(df['time'],Tformat)<F.lit(\"2019-04-01\") ) \n",
    "#     df = df.filter( F.to_date(df['time'],Tformat)>F.lit(\"2015-04-30\") ) #*\n",
    "#     included[3]+=df.count()\n",
    "    \n",
    "#     # Remove timestamp_seen < time and <30 month between\n",
    "#     # Time in unix time: *\n",
    "#     df = df.withColumn('time_unix', F.unix_timestamp('time',Tformat) )\n",
    "#     # timestamp_seen to seconds not ms *\n",
    "#     df = df.withColumn('timestamp_seen', df.timestamp_seen*0.001)\n",
    "#     df = df.filter(df.time_unix < df.timestamp_seen)\n",
    "#     df = df.filter(df.timestamp_seen - df.time_unix < F.lit(30*24*60*60) )\n",
    "#     included[4]+=df.count()\n",
    "    \n",
    "    # Only Keep specific columns:\n",
    "#     df = df.select('useruuid','time','bpm','devices_name') old! (* = new stuff)\n",
    "    df = df.select('useruuid','time','bpm') \n",
    "    \n",
    "    # Create user bucket id from the first two characters of the useruuid\n",
    "    df = df.withColumn('bucket', F.substring('useruuid',0,2))\n",
    "    \n",
    "    # Partion by the bucket ID and save as parquet:\n",
    "    df.write.partitionBy('bucket').mode('append').parquet(path)\n",
    "    \n",
    "    # Clear cache\n",
    "    spark.catalog.clearCache()\n",
    "            \n",
    "    end_time = time.time()    \n",
    "    total_time += (end_time-start_time)    \n",
    "    estimated_time = (total_time / i) * (N-i)    \n",
    "    print(i,'of',N,'/',folder[-6:],\n",
    "          'Total time:',round(total_time,2), 's',\n",
    "          'Estimated time remaining:',round(estimated_time,2), 's', end = '\\r')\n",
    "\n",
    "print(i,'of',N,'/',folder[-6:],\n",
    "      'Total time:',round(total_time,2), 's',\n",
    "      'Estimated time remaining:',round(estimated_time,2), 's    ')\n",
    "print('Done',\" \"*1000)\n",
    "included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2142413238, 2089611309, 2142413231, 2142364977, 2133110355]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included\n",
    "# 2142413238, 2089611309, 2089611302, 2089563279, 2073946706"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1686478668451312"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total processing time:\n",
    "total_time/60/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = '/data/work/shared/s144289/heart_rate/bucket=00/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[useruuid: string, time: string, bpm: int]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = spark.read.format(\"parquet\").load(pp)\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---+\n",
      "|            useruuid|               time|bpm|\n",
      "+--------------------+-------------------+---+\n",
      "|001ecb1d-a276-438...|2016-04-19 20:27:00| 65|\n",
      "|001ecb1d-a276-438...|2016-04-19 20:37:00| 72|\n",
      "|001ecb1d-a276-438...|2016-04-19 20:57:00| 70|\n",
      "|001ecb1d-a276-438...|2016-04-19 21:07:00| 93|\n",
      "|001ecb1d-a276-438...|2016-04-19 21:17:00| 72|\n",
      "|001ecb1d-a276-438...|2016-04-19 21:37:00| 80|\n",
      "|001ecb1d-a276-438...|2016-04-19 21:47:00| 72|\n",
      "|001ecb1d-a276-438...|2016-04-19 22:07:00| 68|\n",
      "|001ecb1d-a276-438...|2016-04-19 22:17:00| 64|\n",
      "|001ecb1d-a276-438...|2016-04-19 22:27:00| 64|\n",
      "+--------------------+-------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dff.show()\n",
    "dff = dff.withColumn('time',F.to_timestamp(dff.time,\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\").alias('time'))\n",
    "dff[dff.useruuid == '001ecb1d-a276-438c-b3d1-b04e38c96881'].orderBy('time').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The rest in specfic for the heart rate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning Phase1\n",
    "\n",
    "* [x] Remove duplicates\n",
    "* [x] Remove <18 years\n",
    "* [x] Track sample sizes/reductions\n",
    "\n",
    "Total time: 19808.41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total user profiles: 6153899\n"
     ]
    }
   ],
   "source": [
    "user_path = '/data/work/shared/safe_data/users/users_demographics_and_country/'\n",
    "df_user = spark.read.format(\"parquet\").load(user_path)\n",
    "df_user = df_user.drop('bucket')\n",
    "print('Number of total user profiles:', df_user.count())\n",
    "\n",
    "# 2019-03. Trust the folders. Changes in June 2016. Maybe focus on after 2016 June"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|birthyear|count|\n",
      "+---------+-----+\n",
      "|     2017|   16|\n",
      "|     2016|  125|\n",
      "|     2015|    4|\n",
      "+---------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user.groupBy('birthyear').count().orderBy('birthyear', ascending = False).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total user profiles < 18 years: 126210\n"
     ]
    }
   ],
   "source": [
    "# df_user = df_user.where((df_user.birthyear <= 1997) | (df_user.birthyear.isNull()))\n",
    "\n",
    "# Dataframe of under age users:\n",
    "df_user = df_user.where((df_user.birthyear > 1997) )\n",
    "under_age_users = [x[0] for x in df_user.select('useruuid').collect()]\n",
    "print('Number of total user profiles < 18 years:', df_user.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 of 256 / bucket=9e/ Total time: 7619.2 s Estimated time remaining: 0.0 s    7 sss\n",
      "Done                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[116920, 116920, 115744, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path of the data:\n",
    "path = '/data/work/shared/s144289/heart_rate/'\n",
    "clean_path = '/data/work/shared/s144289/heart_rate_clean_phase1'\n",
    "\n",
    "Tformat = \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"\n",
    "Included = [0]*4\n",
    "n_users = [0]*4\n",
    "\n",
    "total_time = 0\n",
    "estimated_time = 0\n",
    "N = len(glob.glob(path+'*/'))\n",
    "i=0\n",
    "for folder in glob.glob(path+'*/'):\n",
    "    start_time = time.time()\n",
    "    i+=1\n",
    "    print(i,'of',N,':',folder[-10:], end = '\\r')    \n",
    "    \n",
    "    df_original = spark.read.format(\"parquet\").load(folder)\n",
    "    \n",
    "    Included[0]+=df_original.count()\n",
    "    n_users[0]+=df_original.agg(F.countDistinct('useruuid')).collect()[0][0]\n",
    "    print(i,'of',N,':',folder[-10:], '1/4', end = '\\r')    \n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df_original.dropDuplicates()\n",
    "    \n",
    "    Included[1]+=df.count()\n",
    "    n_users[1]+=df.agg(F.countDistinct('useruuid')).collect()[0][0]\n",
    "    print(i,'of',N,':',folder[-10:], '2/4', end = '\\r')\n",
    "    \n",
    "    # Remove age <18: (Born before 1997 would be +18 in +2015. Null's are also kept)\n",
    "#     df = df.join(df_user, on =['useruuid'], how = 'left_semi') old!\n",
    "#     df = df.filter(~df.useruuid.isin(under_age_users)) # Filter is too slow...\n",
    "    df = df.join(df_user, on =['useruuid'], how = 'left_anti')\n",
    "    \n",
    "    Included[2]+=df.count()\n",
    "    n_users[2]+=df.agg(F.countDistinct('useruuid')).collect()[0][0]\n",
    "    print(i,'of',N,':',folder[-10:], '3/4', end = '\\r')\n",
    "    \n",
    "    # Create user bucket id from the first two characters of the useruuid\n",
    "    # Partion by the bucket ID and save as parquet:\n",
    "    df = df.withColumn('bucket', F.substring('useruuid',0,2)\n",
    "                      ).write.partitionBy('bucket'\n",
    "                                         ).mode('append').parquet(clean_path)\n",
    "    print(i,'of',N,':',folder[-10:], '4/4', end = '\\r')\n",
    "    \n",
    "    # clear GC after each folder (release cast memory)\n",
    "    spark.catalog.clearCache()\n",
    "    \n",
    "            \n",
    "    end_time = time.time()    \n",
    "    total_time += (end_time-start_time)    \n",
    "    estimated_time = (total_time / i) * (N-i)    \n",
    "    print(i,'of',N,'/',folder[-10:], '5/4',\n",
    "          'Total time:',round(total_time,2), 's',\n",
    "          'Estimated time remaining:',round(estimated_time,2), 's', end = '\\r')\n",
    "\n",
    "print(i,'of',N,'/',folder[-10:],\n",
    "      'Total time:',round(total_time,2), 's',\n",
    "      'Estimated time remaining:',round(estimated_time,2), 's    ')\n",
    "print('Done',\" \"*1000)\n",
    "\n",
    "Included\n",
    "n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2073946706, 2041017896, 2035436770, 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Included\n",
    "# [2073946706, 2041017896, 2035436770, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[116920, 116920, 115744, 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users\n",
    "# [116920, 116920, 115744, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning Phase2\n",
    "\n",
    "* [x] Remove other specific users: \n",
    "    * [x] Use-period less than 7 days in a row (max - min time)\n",
    "    * [x] less than 770 samples (5x22x7)\n",
    "* [x] Remove days <20 samples\n",
    "* [x] Track sample sizes/reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115744"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = \"/data/work/shared/s144289/Data_outputs/Users_use_metrics/\"\n",
    "files = glob.glob(folder)\n",
    "df = spark.read.format(\"parquet\").load(files)\n",
    "df = df.drop('n_days')\n",
    "df = df.withColumn('Period',F.col('Period')+1)\n",
    "\n",
    "df_users_exclude = df[( (df['Period'] < 7) & (df['n_samples'] < 5*22*7) )]\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17401"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users_exclude.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/data/work/shared/s144289/Data_outputs/Heart_Rates_per_day_per_user/\"\n",
    "\n",
    "df = spark.read.format(\"parquet\").load(folder)\n",
    "df = df.drop('mean_bpm')\n",
    "df.n_bpm = df.n_bpm.cast('int')\n",
    "\n",
    "df_users_day_exclude = df[df['n_bpm']<20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count(Date)|\n",
      "+-----------+\n",
      "|       1411|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users_day_exclude.agg(F.countDistinct('Date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|count(useruuid)|\n",
      "+---------------+\n",
      "|          88771|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users_day_exclude.agg(F.countDistinct('useruuid')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672107"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users_day_exclude.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 of 256 / bucket=9e/ Total time: 13067.45 s Estimated time remaining: 0.0 s    s ss\n",
      "Done                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "[2035436770, 2032886932, 2028063167]\n",
      "[115744, 100409, 99733]\n"
     ]
    }
   ],
   "source": [
    "# path of the data:\n",
    "path = '/data/work/shared/s144289/heart_rate_clean_phase1/'\n",
    "clean_path = '/data/work/shared/s144289/heart_rate_clean_phase2'\n",
    "\n",
    "Tformat = \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"\n",
    "Included = [0]*3\n",
    "n_users = [0]*3\n",
    "\n",
    "total_time = 0\n",
    "estimated_time = 0\n",
    "N = len(glob.glob(path+'*/'))\n",
    "i=0\n",
    "for folder in glob.glob(path+'*/'):\n",
    "    start_time = time.time()\n",
    "    i+=1\n",
    "    print(i,'of',N,':',folder[-10:], end = '\\r')    \n",
    "    \n",
    "    # dataframe with data (does not load until you execute something below)\n",
    "    df_original = spark.read.format(\"parquet\").load(folder)\n",
    "    \n",
    "    Included[0]+=df_original.count()\n",
    "    n_users[0]+=df_original.agg(F.countDistinct('useruuid')).collect()[0][0]\n",
    "    print(i,'of',N,':',folder[-10:], '1/4', end = '\\r')    \n",
    "\n",
    "    \n",
    "    # Exclude users with less than 7 continous days and 770 samples\n",
    "    df = df_original.join(df_users_exclude[df_users_exclude['bucket'] == folder[-3:-1] ]\n",
    "                          , on ='useruuid', how = 'left_anti')\n",
    "    \n",
    "    Included[1]+=df.count()\n",
    "    n_users[1]+=df.agg(F.countDistinct('useruuid')).collect()[0][0]\n",
    "    print(i,'of',N,':',folder[-10:], '2/4', end = '\\r')   \n",
    "    \n",
    "    \n",
    "    # Exclude user-days with less than 20 samples:\n",
    "    df = df.withColumn('Date',F.to_date('time',Tformat))    \n",
    "    df = df.join(df_users_day_exclude[df_users_day_exclude['bucket'] == folder[-3:-1] ]\n",
    "                 , on =['useruuid','Date'], how = 'left_anti')\n",
    "    \n",
    "    Included[2]+=df.count()\n",
    "    n_users[2]+=df.agg(F.countDistinct('useruuid')).collect()[0][0]\n",
    "    print(i,'of',N,':',folder[-10:], '3/4', end = '\\r')   \n",
    "    \n",
    "    \n",
    "    # Create user bucket id from the first two characters of the useruuid\n",
    "    # Partion by the bucket ID and save as parquet:\n",
    "    df = df.withColumn('bucket', F.substring('useruuid',0,2)\n",
    "                      ).write.partitionBy('bucket'\n",
    "                                         ).mode('append').parquet(clean_path)\n",
    "    print(i,'of',N,':',folder[-10:], '4/4', end = '\\r')\n",
    "    \n",
    "    # clear GC after each folder (release cast memory)\n",
    "    spark.catalog.clearCache()\n",
    "    \n",
    "            \n",
    "    end_time = time.time()    \n",
    "    total_time += (end_time-start_time)    \n",
    "    estimated_time = (total_time / i) * (N-i)    \n",
    "    print(i,'of',N,'/',folder[-10:], '5/4',\n",
    "          'Total time:',round(total_time,2), 's',\n",
    "          'Estimated time remaining:',round(estimated_time,2), 's', end = '\\r')\n",
    "\n",
    "print(i,'of',N,'/',folder[-10:],\n",
    "      'Total time:',round(total_time,2), 's',\n",
    "      'Estimated time remaining:',round(estimated_time,2), 's    ')\n",
    "print('Done',\" \"*1000)\n",
    "\n",
    "print(Included)\n",
    "print(n_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2035436770, 2032886932, 2028063167]\n",
    "# [115744, 100409, 99733]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre phase 2 limit analysis (Users us metrics):\n",
    "* Number of samples\n",
    "* Number of days\n",
    "* Periode (max date to min date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 of 256 / bucket=9e/ Total time: 3813.62 s Estimated time remaining: 0.0 s    ssss\n",
      "Done                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n"
     ]
    }
   ],
   "source": [
    "# path of the data:\n",
    "path = '/data/work/shared/s144289/heart_rate_clean_phase1/'\n",
    "clean_path = '/data/work/shared/s144289/Data_outputs/Users_use_metrics'\n",
    "\n",
    "Tformat = \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"\n",
    "\n",
    "total_time = 0\n",
    "estimated_time = 0\n",
    "N = len(glob.glob(path+'*/'))\n",
    "i=0\n",
    "for folder in glob.glob(path+'*/'):\n",
    "    start_time = time.time()\n",
    "    i+=1\n",
    "    print(i,'of',N,':',folder[-10:], end = '\\r')    \n",
    "    \n",
    "    # dataframe with data (does not load until you execute something below)\n",
    "    df_original = spark.read.format(\"parquet\").load(folder)\n",
    "      \n",
    "    df = df_original.groupby('useruuid').agg(F.count('bpm').alias('n_samples'),\n",
    "                                             F.countDistinct(F.to_date('time')).alias('n_days'),\n",
    "                                             F.datediff(F.max('time'),F.min('time')).alias('Period')\n",
    "                                            )\n",
    "    \n",
    "    \n",
    "    # Create user bucket id from the first two characters of the useruuid\n",
    "    # Partion by the bucket ID and save as parquet:\n",
    "    df = df.withColumn('bucket', F.substring('useruuid',0,2)\n",
    "                      ).write.partitionBy('bucket'\n",
    "                                         ).mode('append').parquet(clean_path)\n",
    "    print(i,'of',N,':',folder[-10:], '4/4', end = '\\r')\n",
    "    \n",
    "    # clear GC after each folder (release cast memory)\n",
    "    spark.catalog.clearCache()\n",
    "            \n",
    "    end_time = time.time()    \n",
    "    total_time += (end_time-start_time)    \n",
    "    estimated_time = (total_time / i) * (N-i)    \n",
    "    print(i,'of',N,'/',folder[-10:], '5/4',\n",
    "          'Total time:',round(total_time,2), 's',\n",
    "          'Estimated time remaining:',round(estimated_time,2), 's', end = '\\r')\n",
    "\n",
    "print(i,'of',N,'/',folder[-10:],\n",
    "      'Total time:',round(total_time,2), 's',\n",
    "      'Estimated time remaining:',round(estimated_time,2), 's    ')\n",
    "print('Done',\" \"*1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "213.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
